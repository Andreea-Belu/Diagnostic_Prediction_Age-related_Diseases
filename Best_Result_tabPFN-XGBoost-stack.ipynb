{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/tabpfn-019-whl/tabpfn-0.1.9-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:26:32.959909Z","iopub.execute_input":"2023-08-01T12:26:32.960348Z","iopub.status.idle":"2023-08-01T12:27:06.647772Z","shell.execute_reply.started":"2023-08-01T12:26:32.960313Z","shell.execute_reply":"2023-08-01T12:27:06.646244Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/tabpfn-019-whl/tabpfn-0.1.9-py3-none-any.whl\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from tabpfn==0.1.9) (1.23.5)\nRequirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from tabpfn==0.1.9) (6.0)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from tabpfn==0.1.9) (2.31.0)\nRequirement already satisfied: scikit-learn>=0.24.2 in /opt/conda/lib/python3.10/site-packages (from tabpfn==0.1.9) (1.2.2)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from tabpfn==0.1.9) (2.0.0+cpu)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn==0.1.9) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn==0.1.9) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn==0.1.9) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn==0.1.9) (2023.5.7)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn==0.1.9) (1.11.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn==0.1.9) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn==0.1.9) (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn==0.1.9) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn==0.1.9) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn==0.1.9) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn==0.1.9) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn==0.1.9) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->tabpfn==0.1.9) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->tabpfn==0.1.9) (1.3.0)\ntabpfn is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"TabPFN (0.1.9) whl needs to be installed in the Data button located on the right side of your notebook from the Kaggle available datasets: https://www.kaggle.com/datasets/carlmcbrideellis/tabpfn-019-whl","metadata":{}},{"cell_type":"code","source":"!mkdir /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n!cp /kaggle/input/tabpfn-019-whl/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:06.650779Z","iopub.execute_input":"2023-08-01T12:27:06.651204Z","iopub.status.idle":"2023-08-01T12:27:09.212033Z","shell.execute_reply.started":"2023-08-01T12:27:06.651163Z","shell.execute_reply":"2023-08-01T12:27:09.210506Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘/opt/conda/lib/python3.10/site-packages/tabpfn/models_diff’: File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"from tabpfn import TabPFNClassifier","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.213486Z","iopub.execute_input":"2023-08-01T12:27:09.213860Z","iopub.status.idle":"2023-08-01T12:27:09.219665Z","shell.execute_reply.started":"2023-08-01T12:27:09.213804Z","shell.execute_reply":"2023-08-01T12:27:09.218659Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder,normalize\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nimport xgboost\nimport inspect\nfrom collections import defaultdict\nfrom tabpfn import TabPFNClassifier\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.222827Z","iopub.execute_input":"2023-08-01T12:27:09.224087Z","iopub.status.idle":"2023-08-01T12:27:09.565712Z","shell.execute_reply.started":"2023-08-01T12:27:09.224038Z","shell.execute_reply":"2023-08-01T12:27:09.564270Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Import datasets in the Input section","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\ntest = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\nsample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\ngreeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.567455Z","iopub.execute_input":"2023-08-01T12:27:09.568492Z","iopub.status.idle":"2023-08-01T12:27:09.608333Z","shell.execute_reply.started":"2023-08-01T12:27:09.568457Z","shell.execute_reply":"2023-08-01T12:27:09.607050Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"As we already seen in the Dataset Visualization notebook the EJ column is categorical and needs to be trasformed to numeric.","metadata":{}},{"cell_type":"code","source":"# Get the first unique category from the 'EJ' column in the 'train' dataset\nfirst_category = train.EJ.unique()[0]\n\n# Convert 'EJ' column in 'train' dataset to binary format: 1 if it matches the first category, 0 otherwise\ntrain.EJ = train.EJ.eq(first_category).astype('int')\n\n# Convert 'EJ' column in 'test' dataset to binary format: 1 if it matches the first category, 0 otherwise\ntest.EJ = test.EJ.eq(first_category).astype('int')","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.609992Z","iopub.execute_input":"2023-08-01T12:27:09.610656Z","iopub.status.idle":"2023-08-01T12:27:09.620685Z","shell.execute_reply.started":"2023-08-01T12:27:09.610614Z","shell.execute_reply":"2023-08-01T12:27:09.619513Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Our target \"Class\" is unbalanced. In order to balance the class distribution by randomly selecting a subset of the majority class (class 0) to match the number of samples in the minority class (class 1) a function is used to perform random undersampling of the train that contains two classes ('Class' column with values 0 and 1).","metadata":{}},{"cell_type":"code","source":"def random_under_sampler(df):\n    # Calculate the number of samples for each label. \n    neg, pos = np.bincount(df['Class'])\n\n    # Choose the samples with class label `1`.\n    one_df = df.loc[df['Class'] == 1] \n    # Choose the samples with class label `0`.\n    zero_df = df.loc[df['Class'] == 0]\n    # Select `pos` number of negative samples.\n    # This makes sure that we have equal number of samples for each label.\n    zero_df = zero_df.sample(n=pos)\n\n    # Join both label dataframes.\n    undersampled_df = pd.concat([zero_df, one_df])\n\n    # Shuffle the data and return\n    return undersampled_df.sample(frac = 1)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.622214Z","iopub.execute_input":"2023-08-01T12:27:09.622593Z","iopub.status.idle":"2023-08-01T12:27:09.637617Z","shell.execute_reply.started":"2023-08-01T12:27:09.622563Z","shell.execute_reply":"2023-08-01T12:27:09.636485Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train_good = random_under_sampler(train)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.639228Z","iopub.execute_input":"2023-08-01T12:27:09.639809Z","iopub.status.idle":"2023-08-01T12:27:09.655302Z","shell.execute_reply.started":"2023-08-01T12:27:09.639773Z","shell.execute_reply":"2023-08-01T12:27:09.654083Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"Now train_good represent undersampled train dataframe with a balanced class distribution, containing equal samples of both classes.","metadata":{}},{"cell_type":"markdown","source":"Preparing the data for training a machine learning model.","metadata":{}},{"cell_type":"code","source":"# Get a list of predictor columns by excluding the 'Class' and 'Id' columns from the 'train' dataset\npredictor_columns = [n for n in train.columns if n != 'Class' and n != 'Id']\n\n# Assign the predictor columns to the variable 'x' for training the model\nx = train[predictor_columns]\n\n# Assign the 'Class' column as the target variable to be predicted and assign it to the variable 'y'\ny = train['Class']","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.656780Z","iopub.execute_input":"2023-08-01T12:27:09.657443Z","iopub.status.idle":"2023-08-01T12:27:09.668946Z","shell.execute_reply.started":"2023-08-01T12:27:09.657399Z","shell.execute_reply":"2023-08-01T12:27:09.668005Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Now, I can use x and y to train your model using an appropriate algorithm.","metadata":{}},{"cell_type":"markdown","source":"Applying two cross-validation strategies using K-Fold in order to get a more reliable estimate of the model's performance on unseen data and avoid overfitting.\n","metadata":{}},{"cell_type":"code","source":"# Importing the necessary modules\nfrom sklearn.model_selection import KFold, GridSearchCV\n\n# Setting up the outer cross-validation strategy with 10 folds\ncv_outer = KFold(n_splits=10, shuffle=True, random_state=0)\n\n# Setting up the inner cross-validation strategy with 5 folds\ncv_inner = KFold(n_splits=5, shuffle=True, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.673858Z","iopub.execute_input":"2023-08-01T12:27:09.674512Z","iopub.status.idle":"2023-08-01T12:27:09.683214Z","shell.execute_reply.started":"2023-08-01T12:27:09.674474Z","shell.execute_reply":"2023-08-01T12:27:09.682116Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Submissions are evaluated using a balanced logarithmic loss. The overall effect is such that each class is roughly equally important for the final score. For this reason I created balanced_log_loss function as implementation of a balanced log loss metric, which can be used for evaluating the performance of binary classifiers. ","metadata":{}},{"cell_type":"code","source":"def balanced_log_loss(y_true, y_pred):\n    # y_true: correct labels 0, 1\n    # y_pred: predicted probabilities of class=1\n    # calculate the number of observations for each class\n    N_0 = np.sum(1 - y_true)\n    N_1 = np.sum(y_true)\n    # calculate the weights for each class to balance classes\n    w_0 = 1 / N_0\n    w_1 = 1 / N_1\n    # calculate the predicted probabilities for each class\n    p_1 = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    p_0 = 1 - p_1\n    # calculate the summed log loss for each class\n    log_loss_0 = -np.sum((1 - y_true) * np.log(p_0))\n    log_loss_1 = -np.sum(y_true * np.log(p_1))\n    # calculate the weighted summed logarithmic loss\n    # (factgor of 2 included to give same result as LL with balanced input)\n    balanced_log_loss = 2*(w_0 * log_loss_0 + w_1 * log_loss_1) / (w_0 + w_1)\n    # return the average log loss\n    return balanced_log_loss/(N_0+N_1)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.684715Z","iopub.execute_input":"2023-08-01T12:27:09.685345Z","iopub.status.idle":"2023-08-01T12:27:09.695664Z","shell.execute_reply.started":"2023-08-01T12:27:09.685303Z","shell.execute_reply":"2023-08-01T12:27:09.694113Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"The Ensemble class is a custom implementation of an ensemble model that combines the predictions of two classifiers: XGBClassifier from XGBoost and TabPFNClassifier from the TabNet framework. The class includes methods for fitting the ensemble and making predictions using the combined classifiers. \n\nThe class's purpose is to create an ensemble of classifiers using the XGBoost classifier and the TabNet classifier. The fit method is responsible for fitting the ensemble using the training data (X and y). The predict_proba method is used for making probability predictions using the trained ensemble.\n\nIt's worth noting that the class handles class imbalance by calculating class imbalance weights based on the predicted probabilities and adjusting the final probabilities accordingly. This can be beneficial when dealing with imbalanced datasets and can help improve the overall performance of the ensemble.","metadata":{}},{"cell_type":"code","source":"class Ensemble():\n    def __init__(self):\n        # Initializing the Ensemble class\n        self.imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n        self.classifiers = [ xgboost.XGBClassifier(),\n                            TabPFNClassifier(N_ensemble_configurations=64)]\n    \n    def fit(self, X, y):\n        # Preparing the data for training the ensemble\n        y = y.values\n        unique_classes, y = np.unique(y, return_inverse=True)\n        self.classes_ = unique_classes\n        \n        # Converting the 'EJ' column to binary format\n        first_category = X.EJ.unique()[0]\n        X.EJ = X.EJ.eq(first_category).astype('int')\n        \n        # Imputing missing values in the data\n        X = self.imputer.fit_transform(X)\n        \n        # Fitting the classifiers in the ensemble\n        for classifier in self.classifiers:\n            if classifier == self.classifiers[1]:\n                # Special case for TabPFNClassifier with overwrite_warning=True\n                classifier.fit(X, y, overwrite_warning=True)\n            else:\n                classifier.fit(X, y)\n     \n    def predict_proba(self, x):\n        # Preprocessing the data for prediction\n        x = self.imputer.transform(x)\n        \n        # Obtaining probabilities from each classifier in the ensemble\n        probabilities = np.stack([classifier.predict_proba(x) for classifier in self.classifiers])\n        \n        # Averaging the probabilities across classifiers\n        averaged_probabilities = np.mean(probabilities, axis=0)\n        \n        # Calculating class imbalance weights\n        class_0_est_instances = averaged_probabilities[:, 0].sum()\n        others_est_instances = averaged_probabilities[:, 1:].sum()\n        \n        # Weighted probabilities based on class imbalance\n        new_probabilities = averaged_probabilities * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(averaged_probabilities.shape[1])]])\n        \n        # Normalizing the probabilities\n        return new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.697267Z","iopub.execute_input":"2023-08-01T12:27:09.697624Z","iopub.status.idle":"2023-08-01T12:27:09.711902Z","shell.execute_reply.started":"2023-08-01T12:27:09.697594Z","shell.execute_reply":"2023-08-01T12:27:09.710971Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# module to visialize the progress as bar, as it gives you a visual indication of the progress and helps estimate the time remaining.\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.713210Z","iopub.execute_input":"2023-08-01T12:27:09.713536Z","iopub.status.idle":"2023-08-01T12:27:09.726046Z","shell.execute_reply.started":"2023-08-01T12:27:09.713508Z","shell.execute_reply":"2023-08-01T12:27:09.725115Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"The training function performs the training and evaluation of a given model using nested cross-validation. Nested cross-validation is used to assess the model's performance by using both an outer cross-validation loop and an inner cross-validation loop. The use of balanced log loss and the transformation of predicted probabilities into class labels with specific thresholds are techniques designed to handle class imbalance in the dataset.","metadata":{}},{"cell_type":"code","source":"def training(model, x, y, y_meta):\n    # Initialize variables for tracking results\n    outer_results = list()\n    best_loss = np.inf\n    split = 0\n    splits = 5\n    \n    # Perform inner cross-validation loop\n    for train_idx, val_idx in tqdm(cv_inner.split(x), total=splits):\n        split += 1\n        \n        # Split data into train and validation sets\n        x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]\n        y_train, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]\n                \n        # Fit the model on the training data\n        model.fit(x_train, y_train)\n        \n        # Make predictions on the validation data\n        y_pred = model.predict_proba(x_val)\n        \n        # Transform predicted probabilities into class labels\n        probabilities = np.concatenate((y_pred[:, :1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n        p0 = probabilities[:, :1]\n        p0[p0 > 0.86] = 1\n        p0[p0 < 0.14] = 0\n        y_p = np.empty((y_pred.shape[0],))\n        \n        # Assign class labels based on transformed probabilities\n        for i in range(y_pred.shape[0]):\n            if p0[i] >= 0.5:\n                y_p[i] = False\n            else:\n                y_p[i] = True\n        y_p = y_p.astype(int)\n        \n        # Calculate the balanced log loss for validation data\n        loss = balanced_log_loss(y_val, y_p)\n\n        # Update the best model and loss if current loss is lower\n        if loss < best_loss:\n            best_model = model\n            best_loss = loss\n            print('best_model_saved')\n        \n        # Append the loss to outer_results list and print current loss\n        outer_results.append(loss)\n        print('>val_loss=%.5f, split = %.1f' % (loss, split))\n    \n    # Print the mean loss across all splits\n    print('LOSS: %.5f' % (np.mean(outer_results)))\n    \n    # Return the best model\n    return best_model","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.727324Z","iopub.execute_input":"2023-08-01T12:27:09.728450Z","iopub.status.idle":"2023-08-01T12:27:09.742623Z","shell.execute_reply.started":"2023-08-01T12:27:09.728379Z","shell.execute_reply":"2023-08-01T12:27:09.741579Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"greeks.csv is Supplemental metadata, only available for the training set.\nHere, the 'times' Series will contain datetime ordinal values for non-'Unknown' dates and NaN for 'Unknown' dates, which makes it suitable for further data analysis or modeling.","metadata":{}},{"cell_type":"code","source":"# Importing the necessary module\nfrom datetime import datetime\n\n# Copying the 'Epsilon' column to 'times'\ntimes = greeks.Epsilon.copy()\n\n# Converting non-'Unknown' values in 'Epsilon' column to datetime ordinal\ntimes[greeks.Epsilon != 'Unknown'] = greeks.Epsilon[greeks.Epsilon != 'Unknown'].map(lambda x: datetime.strptime(x, '%m/%d/%Y').toordinal())\n\n# Setting 'Unknown' values in 'Epsilon' column to NaN\ntimes[greeks.Epsilon == 'Unknown'] = np.nan","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.744465Z","iopub.execute_input":"2023-08-01T12:27:09.745186Z","iopub.status.idle":"2023-08-01T12:27:09.769616Z","shell.execute_reply.started":"2023-08-01T12:27:09.745142Z","shell.execute_reply":"2023-08-01T12:27:09.768367Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Data preparation for making predictions on the 'test' dataset using a machine learning model. It combines the 'train' dataset and the 'times' column along the columns axis, prepares the 'test_predictors' dataset, and concatenates it with an additional column derived from the 'train_pred_and_time.Epsilon' column.","metadata":{}},{"cell_type":"code","source":"# Concatenate the 'train' dataset and 'times' column along the columns axis\ntrain_pred_and_time = pd.concat((train, times), axis=1)\n\n# Select predictor columns from the 'test' dataset\ntest_predictors = test[predictor_columns]\n\n# Convert the 'EJ' column in 'test_predictors' to binary format\nfirst_category = test_predictors.EJ.unique()[0]\ntest_predictors.EJ = test_predictors.EJ.eq(first_category).astype('int')\n\n# Concatenate 'test_predictors' and a column of zeros with a value equal to the maximum value in 'train_pred_and_time.Epsilon' plus 1\ntest_pred_and_time = np.concatenate((test_predictors, np.zeros((len(test_predictors), 1)) + train_pred_and_time.Epsilon.max() + 1), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.771526Z","iopub.execute_input":"2023-08-01T12:27:09.772288Z","iopub.status.idle":"2023-08-01T12:27:09.786381Z","shell.execute_reply.started":"2023-08-01T12:27:09.772232Z","shell.execute_reply":"2023-08-01T12:27:09.785194Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Random oversampling using the RandomOverSampler to address the class imbalance in the 'train_pred_and_time' dataset with respect to the 'greeks.Alpha' target variable. Random oversampling is a technique used to increase the number of instances of the minority class by randomly duplicating some of its samples until it reaches the same number of samples as the majority class.","metadata":{}},{"cell_type":"code","source":"# Initialize the RandomOverSampler with a random state of 0\nros = RandomOverSampler(random_state=0)\n\n# Perform oversampling on the 'train_pred_and_time' dataset and 'greeks.Alpha' target variable\ntrain_ros, y_ros = ros.fit_resample(train_pred_and_time, greeks.Alpha)\n\n# Print the value counts of the original 'Alpha' classes\nprint('Original dataset shape')\nprint(greeks.Alpha.value_counts())\n\n# Print the value counts of the resampled 'Alpha' classes\nprint('Resample dataset shape')\nprint(y_ros.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.787704Z","iopub.execute_input":"2023-08-01T12:27:09.788259Z","iopub.status.idle":"2023-08-01T12:27:09.834800Z","shell.execute_reply.started":"2023-08-01T12:27:09.788225Z","shell.execute_reply":"2023-08-01T12:27:09.833575Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Original dataset shape\nA    509\nB     61\nG     29\nD     18\nName: Alpha, dtype: int64\nResample dataset shape\nB    509\nA    509\nD    509\nG    509\nName: Alpha, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Make variables x_ros and y_ are created for training a machine learning model using the oversampled dataset (train_ros) to predict the target variable Class.\n    \n* x_ros: A DataFrame containing the input features (predictor columns) from the oversampled dataset. It is ready to be used as input for training a machine learning model.\n\n* y_: A Series containing the target variable 'Class' (binary labels) from the oversampled dataset. It represents the class labels corresponding to the input features in x_ros and is used as the target variable during model training.","metadata":{}},{"cell_type":"code","source":"x_ros = train_ros.drop(['Class', 'Id'],axis=1)\ny_ = train_ros.Class","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.836950Z","iopub.execute_input":"2023-08-01T12:27:09.837308Z","iopub.status.idle":"2023-08-01T12:27:09.847416Z","shell.execute_reply.started":"2023-08-01T12:27:09.837269Z","shell.execute_reply":"2023-08-01T12:27:09.845942Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"Variable yt is created as an instance of the Ensemble class. ","metadata":{}},{"cell_type":"code","source":"yt = Ensemble()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:09.848922Z","iopub.execute_input":"2023-08-01T12:27:09.849250Z","iopub.status.idle":"2023-08-01T12:27:10.280148Z","shell.execute_reply.started":"2023-08-01T12:27:09.849223Z","shell.execute_reply":"2023-08-01T12:27:10.278258Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Loading model that can be used for inference only\nUsing a Transformer with 25.82 M parameters\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The training function is used to train the Ensemble model yt on the balanced dataset x_ros and its corresponding target labels y_. The function uses nested cross-validation to assess the model's performance and returns the best trained model m. Now I have the best trained ensemble model stored in the variable m, and you can use this model to make predictions on new data or perform further analysis.","metadata":{}},{"cell_type":"code","source":"m = training(yt,x_ros,y_,y_ros)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:27:10.281634Z","iopub.execute_input":"2023-08-01T12:27:10.282534Z","iopub.status.idle":"2023-08-01T12:42:49.423434Z","shell.execute_reply.started":"2023-08-01T12:27:10.282499Z","shell.execute_reply":"2023-08-01T12:42:49.422345Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efbcffcb396b46d4866cb14f9cc71a31"}},"metadata":{}},{"name":"stdout","text":"best_model_saved\n>val_loss=0.00000, split = 1.0\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/2191353139.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X.EJ = X.EJ.eq(first_category).astype('int')\n","output_type":"stream"},{"name":"stdout","text":"best_model_saved\n>val_loss=0.00000, split = 2.0\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/2191353139.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X.EJ = X.EJ.eq(first_category).astype('int')\n","output_type":"stream"},{"name":"stdout","text":">val_loss=0.13011, split = 3.0\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/2191353139.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X.EJ = X.EJ.eq(first_category).astype('int')\n","output_type":"stream"},{"name":"stdout","text":">val_loss=0.39033, split = 4.0\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/2191353139.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X.EJ = X.EJ.eq(first_category).astype('int')\n","output_type":"stream"},{"name":"stdout","text":">val_loss=0.12761, split = 5.0\nLOSS: 0.12961\n","output_type":"stream"}]},{"cell_type":"code","source":"#calculates the relative frequency (proportion) of each class in the target variable y_. It shows the percentage of each class in the balanced target variable after random oversampling.\ny_.value_counts()/y_.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:42:49.424968Z","iopub.execute_input":"2023-08-01T12:42:49.425296Z","iopub.status.idle":"2023-08-01T12:42:49.433756Z","shell.execute_reply.started":"2023-08-01T12:42:49.425265Z","shell.execute_reply":"2023-08-01T12:42:49.432542Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"1    0.75\n0    0.25\nName: Class, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"y_pred is used to store the probability predictions made by the trained ensemble model m on the test dataset test_pred_and_time. The predict_proba method of the m model is used to obtain the probability predictions for each sample in the test dataset.","metadata":{}},{"cell_type":"code","source":"y_pred = m.predict_proba(test_pred_and_time)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:42:49.435119Z","iopub.execute_input":"2023-08-01T12:42:49.435429Z","iopub.status.idle":"2023-08-01T12:45:25.061247Z","shell.execute_reply.started":"2023-08-01T12:42:49.435402Z","shell.execute_reply":"2023-08-01T12:45:25.060157Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The predicted probabilities y_pred obtained from the ensemble model for the test dataset are transformed into class probabilities. Then, thresholding is applied to assign class labels based on the probabilities. The variable p0 will contain the predicted class labels based on the thresholding of the predicted probabilities.","metadata":{}},{"cell_type":"code","source":"# Transform the predicted probabilities into class probabilities\nprobabilities = np.concatenate((y_pred[:, :1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n\n# Extract the column of probabilities for class 0\np0 = probabilities[:, :1]\n\n# Threshold the probabilities to assign class labels\np0[p0 > 0.62] = 1  # Assign values above 0.63 as class 1\np0[p0 < 0.26] = 0  # Assign values below 0.26 as class 0","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:45:25.065711Z","iopub.execute_input":"2023-08-01T12:45:25.066138Z","iopub.status.idle":"2023-08-01T12:45:25.073279Z","shell.execute_reply.started":"2023-08-01T12:45:25.066102Z","shell.execute_reply":"2023-08-01T12:45:25.072376Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"Prepare the submission file for the binary classification task. The DataFrame will have three columns: 'Id', 'class_0', and 'class_1'.","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame for the submission with the 'Id' column from the 'test' dataset\nsubmission = pd.DataFrame(test[\"Id\"], columns=[\"Id\"])\n\n# Add the 'class_0' column containing the thresholded predictions for class 0\nsubmission[\"class_0\"] = p0\n\n# Add the 'class_1' column containing the complement of the thresholded predictions for class 0\nsubmission[\"class_1\"] = 1 - p0\n\n# Save the submission DataFrame to a CSV file named 'submission.csv' without including the index column\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:45:25.074981Z","iopub.execute_input":"2023-08-01T12:45:25.075648Z","iopub.status.idle":"2023-08-01T12:45:25.092337Z","shell.execute_reply.started":"2023-08-01T12:45:25.075614Z","shell.execute_reply":"2023-08-01T12:45:25.091319Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"#Visualization of submission table\nsubmission_df = pd.read_csv('submission.csv')\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2023-08-01T12:45:25.093726Z","iopub.execute_input":"2023-08-01T12:45:25.094537Z","iopub.status.idle":"2023-08-01T12:45:25.119372Z","shell.execute_reply.started":"2023-08-01T12:45:25.094471Z","shell.execute_reply":"2023-08-01T12:45:25.118061Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"             Id  class_0  class_1\n0  00eed32682bb      0.5      0.5\n1  010ebe33f668      0.5      0.5\n2  02fa521e1838      0.5      0.5\n3  040e15f562a2      0.5      0.5\n4  046e85c7cc7f      0.5      0.5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>class_0</th>\n      <th>class_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00eed32682bb</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>010ebe33f668</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>02fa521e1838</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>040e15f562a2</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>046e85c7cc7f</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}